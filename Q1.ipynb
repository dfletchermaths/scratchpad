{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k - anonymity analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1a)\n",
    "Randomly generate a dataset (dataframe) with eight columns and 50,000 rows. \n",
    "Each column should be a categorical variable (of arbitrary name) with three levels (of arbitrary names) in approximately equal proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Comment: We will randomly assign a value to each row, for each variable. \n",
    "##This simulates sampling from a larger population, so will reflect a realistic use-case.\n",
    "##(Ie, in reality there will never be a *precisely* even spread of a variable between levels.\n",
    "\n",
    "##To simplify matters we will use the same numerical labels for each column.\n",
    "\n",
    "#Importing standard libraries\n",
    "library(plyr)\n",
    "library(dplyr)\n",
    "library(tibble)\n",
    "library(tidyr)\n",
    "\n",
    "\n",
    "##Choose variable names\n",
    "variable_names <- LETTERS[1:8] #equivalent to names(random_set) <- c(\"\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",H\")\n",
    "\n",
    "#Choose a seed to make our calculations reproductible.\n",
    "set.seed(1) \n",
    "\n",
    "#Create random dataset\n",
    "df <- data.frame(replicate(8,sample(1:3,50000,rep=TRUE)))\n",
    "names(df) <- variable_names \n",
    "\n",
    "#Add row_id column\n",
    "df <- add_column(df, row_id = 0, .before = 1)\n",
    "df$row_id <- seq.int(nrow(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  row_id A B C D E F G H\n",
      "1      1 1 1 1 1 2 3 3 2\n",
      "2      2 3 3 2 3 3 1 3 3\n",
      "3      3 1 1 1 2 1 3 1 1\n",
      "4      4 2 1 3 2 3 1 1 1\n",
      "5      5 1 2 3 3 2 3 3 1\n",
      "6      6 3 2 1 1 3 1 1 1\n"
     ]
    }
   ],
   "source": [
    "print(head(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1b)\n",
    "Verify that the proportions of each value are similar for each of the eight columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      A  freq\n",
      " [1,] 1 16651\n",
      " [2,] 2 16566\n",
      " [3,] 3 16783\n",
      " [4,] 1 16737\n",
      " [5,] 2 16675\n",
      " [6,] 3 16588\n",
      " [7,] 1 16877\n",
      " [8,] 2 16553\n",
      " [9,] 3 16570\n",
      "[10,] 1 16754\n",
      "[11,] 2 16585\n",
      "[12,] 3 16661\n",
      "[13,] 1 16779\n",
      "[14,] 2 16566\n",
      "[15,] 3 16655\n",
      "[16,] 1 16646\n",
      "[17,] 2 16632\n",
      "[18,] 3 16722\n",
      "[19,] 1 16707\n",
      "[20,] 2 16514\n",
      "[21,] 3 16779\n",
      "[22,] 1 16661\n",
      "[23,] 2 16679\n",
      "[24,] 3 16660\n"
     ]
    }
   ],
   "source": [
    "#Create empty dataframe to store results\n",
    "proportions <- matrix(vector(), nrow = 0, ncol = 2) \n",
    "\n",
    "#Loop through each variable, appending to the empty dataframe\n",
    "for (i in variable_names){\n",
    "  \n",
    "    #Append the number of 'A', 'B' and 'C' values for each column.\n",
    "    proportions <- rbind(proportions, as.matrix( count(df, i) ) ) \n",
    "  \n",
    "}\n",
    "\n",
    "# Print the proportions of each column and variable (the first column being the top 3 rows,\n",
    "# the second column the next three rows, etc).\n",
    "print(proportions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n",
       "  16514   16587   16661   16667   16726   16877 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate summary statistics for these proportions.\n",
    "summary(proportions[,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a maximum variation of less than 1%, so the proportions of each values are similar \n",
    "for each of the eight columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1c)\n",
    "How many unique rows (i.e., permutations of category levels) are possible?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 6561\n"
     ]
    }
   ],
   "source": [
    "##With 8 columns, each having 3 possibilities, there are 3^8 possibilities.\n",
    "print(3^8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1d)\n",
    "Write some code to produce a table and graph which show the frequencies (numbers of groups) by permutation group sizes (up to group size of 10). That is, how many groups are unique combinations (group size = 1), how many groups are made up of a pair of matching combinations (group size = 2), how many groups are made up three the same, etc?\n",
    "For example, in the table below left (conveniently ordered) of three columns and eight rows, there is one unique row (one group of 1), four rows in pairs (two groups of 2) and three rows in groups of three (one group of 3). Each of the variables in the table has three levels; a, b and c. The table on the right shows the corresponding frequency table, which you should produce for the data you created in part a).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# A tibble: 10 x 2\n",
      "   Result Count\n",
      "    <int> <int>\n",
      " 1      1    25\n",
      " 2      2    90\n",
      " 3      3   245\n",
      " 4      4   444\n",
      " 5      5   672\n",
      " 6      6   917\n",
      " 7      7   949\n",
      " 8      8   892\n",
      " 9      9   739\n",
      "10     10   621\n"
     ]
    }
   ],
   "source": [
    "##Step 1. Create a new column with the concatenation of all variables (ie, in the example above,\n",
    "## this column would start; aab, aab, aab, bca, ...)\n",
    "\n",
    "df <- df %>% \n",
    "  unite(Concat_Columns,variable_names , sep = \"\", remove = FALSE)\n",
    "\n",
    "\n",
    "##Step 2. Group by that column, producing a new result column 'Result'.\n",
    "\n",
    "temp_df <- df %>% dplyr::group_by(Concat_Columns) %>% dplyr::summarize(Result = n()) %>% arrange(desc(Result)) \n",
    "\n",
    "\n",
    "##Step 3. Group by our new 'Result' column, to get the final output.\n",
    "output <- temp_df %>% dplyr::group_by(Result) %>% dplyr::summarize(Count = n()) %>% arrange(Result) \n",
    "rm(temp_df)\n",
    "\n",
    "output <- output %>% dplyr::filter(Result <= 10)\n",
    "#Print output table of frequencies (numbers of groups) by permutation group sizes (up to 10).\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1e)\n",
    "\n",
    "Comment upon the distribution of group sizes in d)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are 25 groups which are uniquely identifible.\n",
    "The count increases up to a group size of 7, before slowly decreasing.\n",
    "If this was not an ad-hoc analysis, we would repeat this investigation multiple times to verify that these figures remain stable (aka using 'Monte Carlo' simulation techniques).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1f)\n",
    "If your random variables were, in fact, meaningful information on individuals, which\n",
    "group sizes are of most concern from a privacy perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The obvious issue is group sizes of a single household.\n",
    "Imagine that the above dataset is a list of publicly available information about individuals in a country,\n",
    "which is associated with a report on health insurance claims (which contains private information, such as health conditions).\n",
    "This report would contain a list of individuals with severe health issues, and which of the eight categories they fall into.\n",
    "\n",
    "An attacker could then use the publicly available information to correctly identify what individual household is being referred to,\n",
    "and thus who has the condition.\n",
    "\n",
    "However the issue is not limited to just group sizes of a single household.\n",
    "Imagine a situation where an attacker can use the publicly available information to narrow down the list of people to 3 people\n",
    "(equivalent to a group size of 3).\n",
    "Then the attacker would know there is a 33% chance that any individual in the group has the medical condition, and could take advantage\n",
    "of that. For example, a insurance company could raise premiums, or a potential employer could discrimanate again all individuals\n",
    "in that group of 3.\n",
    "\n",
    "A real-life example of this would be estimating someones ethnicity from their postcode.\n",
    "\n",
    "https://www.ons.gov.uk/aboutus/transparencyandgovernance/freedomofinformationfoi/ethnicgroupstatisticsatpostcodelevel\n",
    "\n",
    "It is also feasible for small groups to be entirely full of people with a medical condition.\n",
    "Imagine that there is a 30% chance that someone has a medical condition.\n",
    "In our example, there are 93 groups of 2 households. There is a 9% chance (30% * 30% ) that two people in any group of 2 both have the condition. With 93 groups, it is highly likely that at least one group has both households with the condition (and thus in our list).\n",
    "This means that an attacker could again identify people with that medical condition with 100% certainity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1g)\n",
    "Consider the effect of missing data in the dataset you created in Part a). How might\n",
    "this complicate the production of a frequency table of group sizes in Part d)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The obvious question here is what is meant by missing data, and how precise does the frequency table have to be?\n",
    "\n",
    "If the creator of the list not have the data (ie, bank account details or other private information),\n",
    "or does the individual row not have a valid value for that data (ie, age of spouse for a single person).\n",
    "\n",
    "If no valid value exists, we could replace with a placeholder value not used elsewhere in the data (ie for\n",
    " age of spouse, use '-1'). We should be cautious of privacy effects though, since that would effectively remove one variable/column from the data, making it easily to identify individuals.\n",
    " \n",
    "If the information is unknown, then one option would be to estimate the value (aka 'impute the data').\n",
    "For example imagine that an individual has a value of 'A' for the first 7 columns, and an unknown eighth column.\n",
    "To fill in the remaining column we could gather together all people who also had 'A' in the first 7 columns, but had a valid result in the last column.\n",
    "\n",
    "We could then randomly reselect a replacement value from those people. \n",
    "This could be useful for columns containing data which will be heavily dependant on previous columns. For example igf the 7th column is what county a person lives in, and you want a good placeholder for the city.\n",
    "Alternatively, we could select from all valid values in the data for all people (useful for, say, age).\n",
    "\n",
    "If estimating the data is not acceptable, we could seperately return the values with missing data as exceptions, and only\n",
    "produce a table for values with all data. Alternatively we could map missing data to a seperate level, and proceed as normal.\n",
    "Obviously if exceptions are rare, that could produce privacy issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1h)\n",
    "Imagine the code that you wrote for Part d) was to be deployed in an automated system\n",
    "that our customers could use independently, on potentially large volumes of data.\n",
    "Describe how you might deploy the code, and what additional considerations you might\n",
    "have or any changes to the code you might make. Note: it is not necessary to provide\n",
    "another version of the code used for 1 d)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main issues with deploying this code in an automated system.\n",
    "\n",
    "Firstly it would have to be generalised for any input.\n",
    "In particular, variable values would have to be shortened for the concat function to work without\n",
    "overrunning a valid size of a variable name (or empty variable names causing issues).\n",
    "I suggest mapping all of the values in each column to unique integers.\n",
    "\n",
    "There may also be issues with customers trying to put too much data into the automated system. \n",
    "For example, a customer testing a file with a million rows and a thousand columns.\n",
    "A quick fix to this would be limiting the size of the file that this system would accept, or ending the process with an error message if the process takes too long.\n",
    "\n",
    "As implied in Q1g), they may pass us a file with missing data.\n",
    "There are several different ways we could deal with missing data as per my answer to Q1g).\n",
    "We could add a suggested list of methods (imputting random values, imputting the most common value, excluding rows with missing values, or treating a missing value as it's own level).\n",
    "However my gut feel is that a conversation would have to be had with the end user to understand their use-case.\n",
    "Similarly there will almost certainly be issues with misspellings in the raw data presented (ie, Birmingham v Birmimgham), which requires understanding of the business case as to whether it's helpful to 'fix' mispellings.\n",
    "\n",
    "\n",
    "My recommendation for deploying the code would be a cloud computing API, such as one created from Azure's Machine Learning Studio. This removes most of the data engineering associated with setting up and maintaining the service.\n",
    "\n",
    "--\n",
    "One specific issue with an extremely large number of columns is the 'concat' column used in the code.\n",
    "Since it combines the data from all other columns, we might start getting issues with the 'concat' column being of arbitary size. \n",
    "A solution to this would be introducing a hash function to map the concat values into a hash table. This would have a small chance of mapping two different values to the same value. However this could only make groups bigger, not smaller.\n",
    "Hence any small groups in the data would still correspond to real issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
